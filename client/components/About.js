import React from 'react';
import Container from 'react-bootstrap/Container';
import Accordion from 'react-bootstrap/Accordion';
import Row from 'react-bootstrap/Row';
import Col from 'react-bootstrap/Col';
import { Layout } from './Layout.js';

import ReactMarkdown from 'react-markdown';
import remarkBreaks from 'remark-breaks';

class About extends React.Component {
  constructor(props) {
    super(props);
    this.state = {};
  }

  render() {
    return (
      <Layout>
        <Container className="about-view">
          <Row>
          <h1>
            About
          </h1>
          </Row>
          <Row>
            <Accordion defaultActiveKey="0">
              <Accordion.Item eventKey="0">
                <Accordion.Header>Fourier App</Accordion.Header>
                <Accordion.Body>
                  <ReactMarkdown remarkPlugins={[remarkBreaks]}>
  {`This app began as a source code summarizer. After a month or so of development, I finally learned why it hasn't been done before.

  When you start off a machine learning project with tensorflow or keras as a beginner, reading the documentation
  itself is a monumental task. What they don't mention in the documentation is that in order for your project
  to function even at a basic level, it needs to have an extraordinary amount of data.
  
  At the very least, my model required a dataset of around 10,000 entries just to show that it was learning and adapting properly.
  
  I spent about a month tweaking, configuring, and toiling away. After a few weeks of googling and Stackoverflow,
  
  I discovered there was no way I was going to see results as long as my dataset was below 10,000 training samples.
  
  The particular data that Fourier required was "handcrafted" code examples with the target summaries that accompanied them.
  (I tried to implement the model with only 60 training samples, thinking that the problem could be scaled down that far to still see results.)
  
  Don't get me wrong, the model worked. I know this because I exported and refactored it to run on different data, 
  plain human language translations, and it worked perfectly.
  
  It just wasn't showing any signs of learning with only 60 samples when run on programming languages.
  
  If you'd like to learn about how development for the original Fourier went, check out the "Fourier Classic" description below.`}
                  </ReactMarkdown>
                </Accordion.Body>
              </Accordion.Item>
              <Accordion.Item eventKey="1">
                <Accordion.Header>Fourier Classic</Accordion.Header>
                <Accordion.Body>
                  <ReactMarkdown>
{`Fourier is a source code summarizer that uses natural language processing to summarize code into human readable phrases. 
It is a simple tool designed to increase the speed at which developers learn and understand code written by others.

Fourier is not meant to replace the human reader, but to augment them. This tool hopefully makes understanding convoluted code bases 
and complex programming languages much easier and much less time-intensive.

##### Current Functionality
So far Fourier only processes very simple javascript code. At its current phase, it merely translates source code directly
into "legible" English snippets. The snippets themselves are too literal and basic to be useful for summarizing, 
but they will eventually become a stepping stone.

In the future, a second model will be built to summarize these first snippets to produce actual summaries.

It is also difficult to get hold of suitable training data since there are virtually no random source code generators available open source. 
("Random" meaning entirely unrelated programs. Current source generators involve compiling off an existing codebase.)

Fourier is still in its proof of concept phase and is NOT built for performance.

The NLP translation model is built from scratch using Tensorflow and Keras.
The model is trained so far using a manually crafted dataset of 60 entries.
The data itself is comprised of Abstract Syntax Trees generated by the [Acorn](https://www.npmjs.com/package/acorn) npm package.
The ASTs are parsed, trimmed, and processed into a digestible form for the model.

The model itself is classified as a Sequence-to-Sequence architecture.
It features an encoder and decoder, each a composite of Embedding, LSTM, and Dense neural network layers.
The code for the model and its performance metrics can be found on Kaggle at [Fourier](https://www.kaggle.com/cf1111/fourier2/notebook)

When code (in the form of an AST) is submitted to the Fourier backend, it goes through further transformations
that eventually convert it into integer values that the model then reads to predict the human words it has learned to
correlate with those sequences.
The model then returns its own predicted sequence of english words that match the input code sequence.

##### Training data sample:
~~~
code: console.log("hello");
ast: {"type":"Program","body":[{"type":"ExpressionStatement","expression":{"type":"CallExpression","callee":{"type":"MemberExpression","object":{"type":"Identifier","name":"console"},"property":{"type":"Identifier","name":"log"},"computed":false,"optional":false},"arguments":[{"type":"Literal","value":"hello","raw":"\"hello\""}],"optional":false}}],"sourceType":"script"}
~~~`}
                  </ReactMarkdown>
                </Accordion.Body>
              </Accordion.Item>
            </Accordion>
          </Row>
        </Container>
      </Layout>
    );
  }
}

export { About };